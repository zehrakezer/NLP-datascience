{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# squad veriseti zerinde ince ayar yapılmış bert fit modeli\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "#bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#soru cevaplama için ince ayar yapılmjş bert modeli\n",
    "model= BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "#cevapları tahmin eden fonksiyon\n",
    "def predict_answer(context, question):\n",
    "    \"\"\"\n",
    "        context = metin\n",
    "        question = soru\n",
    "        Amaç = metin içerisindeki soruyu bulmak\n",
    "\n",
    "        1)tokenize\n",
    "        2)metnin içerisinde soruyu ara\n",
    "        3)metnin içerisinde sorunun cevabını nerelerde olabilreceğinin scorelarını return et\n",
    "        4) skorladan tokenların indexlerinin hesapladdık\n",
    "        5)tokenları bulduk yani cevabı bulduk\n",
    "        6) tokenların okunabilir olması için tokenlardan stringe çevirdik\n",
    "    \"\"\"\n",
    "\n",
    "    #metni ve soruyu tokenlara ayıralım ve modele uygun hale getirelim\n",
    "    encoding = tokenizer.encode_plus(question,context,return_tensors = \"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    #giriş tensorleri hazırla\n",
    "    input_ids = encoding[\"input_ids\"] #tokenlerin idleri\n",
    "    attention_masks = encoding[\"attention_mask\"] #hangi tokenların dikkate alınacağını belirler\n",
    "\n",
    "    #modeli çalıştır ve skorları hesapla\n",
    "    with torch.no_grad():\n",
    "        start_scores, end_scores = model(input_ids,attention_masks,return_dict=False)\n",
    "\n",
    "    #en yüksek olasılığa sahip start ve end indekslerini hesaplıyor\n",
    "    start_index= torch.argmax(start_scores, dim=1).item() #başlangıç indeksleri\n",
    "    end_index = torch.argmax(end_scores,dim=1).item() #bitiş indexi\n",
    "\n",
    "    #token idlerini kullanarak cevap metinin elde edimi\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index+1])\n",
    "\n",
    "    #tokenları birleştir ve okunabilir hale getir\n",
    "    answer= tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    return answer\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the capital of france?\"\n",
    "context = \"France, offically the French Republic, is a country of capital Paris\"\n",
    "\n",
    "answer = predict_answer(context,question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT ile SORU CEVAPLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kezer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer,TFGPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kezer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name= \"gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model= TFGPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(context,question):\n",
    "\n",
    "    input_text = f\"Question: {question}, Context: {context}.Please answer the question according the context\"\n",
    "\n",
    "    #tokenlaştırma\n",
    "    inputs = tokenizer.encode(input_text,return_tensors=\"pt\")\n",
    "\n",
    "    #modeli çakıştır\n",
    "    with torch.no_grad():\n",
    "        outputs= model.generate(inputs,max_length = 500)\n",
    "\n",
    "    #üretilen yanıtı decode et\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True) #merhaba<EOS><PAD>\n",
    "\n",
    "    #yanıtları ayıklayalım\n",
    "    answer = answer.split(\"Answer\")[-1].strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "': The capital of france is the capital of France.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the capital of france?\"\n",
    "context = \"France, offically the French Republic, is a country of capital Paris\"\n",
    "\n",
    "answer = generate_answer(context,question)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
